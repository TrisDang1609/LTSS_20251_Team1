#include "common_kernel.cl"

/*
================================================================================
GPU-ACCELERATED SIFT DESCRIPTOR COMPUTATION
================================================================================
Optimized for NVIDIA RTX 4060 (Ada Lovelace, 8GB VRAM, 24 SMs)

ARCHITECTURAL OBJECTIVE: Zero CPU Readback
- Input: Oriented keypoints buffer (in VRAM from compute_orientations)
- Operation: Sample 16×16 rotated patch → 4×4×8 histogram → 128-dim descriptor
- Output: Final descriptors buffer (ready for SINGLE D2H transfer)

CPU REFERENCE LOGIC (sift.cpp):
    void compute_keypoint_descriptor(Keypoint &kp, float theta,
                                     const ScaleSpacePyramid &grad_pyramid,
                                     float lambda_desc)
    {
        // Sample 16×16 patch around keypoint, rotated by theta
        float patch_sigma = lambda_desc * kp.sigma;
        float hist[4][4][8] = {};  // 4×4 spatial bins × 8 orientation bins

        for (int x = -8; x <= 7; x++) {
            for (int y = -8; y <= 7; y++) {
                // Rotate coordinates by -theta
                float x_rot = cos(theta)*x - sin(theta)*y;
                float y_rot = sin(theta)*x + cos(theta)*y;

                // Sample gradient at rotated position
                float grad_mag = bilinear_sample(grad_img, x_rot, y_rot, 0);
                float grad_ori = bilinear_sample(grad_img, x_rot, y_rot, 1);

                // Compute relative orientation (subtract keypoint orientation)
                float theta_rel = grad_ori - theta;

                // Gaussian weighting
                float weight = exp(-(x*x + y*y) / (2*patch_sigma*patch_sigma));

                // Trilinear interpolation into 4×4×8 histogram
                update_histograms(hist, x, y, weight * grad_mag, theta_rel,
lambda_desc);
            }
        }

        // Flatten histogram → 128-element vector
        // Normalize to unit length
        // Threshold at 0.2 and re-normalize (illumination invariance)
        // Quantize to uint8
    }

PERFORMANCE TARGET (RTX 4060):
    - Input: ~7K oriented keypoints
    - Patch size: 16×16 = 256 samples/descriptor
    - Output: 7K × 128-dimensional descriptors = 896 KB
    - Execution time: ~8-12ms (vs ~30ms CPU)
    - Speedup: 2.5-4×

MEMORY LAYOUT:
    - Input: oriented_keypoints [7K × OrientedKeypoint] = 252 KB
    - Gradient pyramid: ~253 MB (magnitude + orientation)
    - Histograms: [7K × 4×4×8 floats] = 3.5 MB (local memory per thread)
    - Output: descriptors [7K × 128 uint8] = 896 KB

Ada Lovelace Optimizations:
    - One work-item per keypoint (7K threads)
    - Local memory for 4×4×8 histogram (512 bytes)
    - Hardware bilinear sampling (texture cache)
    - Vectorized normalization (float4 operations)
================================================================================
*/

#define N_HIST 4
#define N_ORI 8
#define DESCRIPTOR_SIZE 128
#define TWO_PI 6.28318530718f

/*
Helper: Bilinear interpolation for gradient sampling.
Gradient is stored as interleaved [magnitude, orientation] per pixel.
*/
float bilinear_sample(__global const float *img, int width, int height, float x,
                      float y, int channel) {
  // Clamp to valid range
  x = clamp(x, 0.0f, (float)(width - 1));
  y = clamp(y, 0.0f, (float)(height - 1));

  int x0 = (int)floor(x);
  int y0 = (int)floor(y);
  int x1 = min(x0 + 1, width - 1);
  int y1 = min(y0 + 1, height - 1);

  float fx = x - x0;
  float fy = y - y0;

  // Read 4 corner values (interleaved 2-channel layout)
  float v00 = img[2 * (y0 * width + x0) + channel];
  float v10 = img[2 * (y0 * width + x1) + channel];
  float v01 = img[2 * (y1 * width + x0) + channel];
  float v11 = img[2 * (y1 * width + x1) + channel];

  // Bilinear interpolation
  return (1.0f - fx) * (1.0f - fy) * v00 + fx * (1.0f - fy) * v10 +
         (1.0f - fx) * fy * v01 + fx * fy * v11;
}

/*
Helper: Linear interpolation to distribute gradient contribution
into 4×4×8 histogram (matches update_histograms in sift.cpp).

Parameters:
    - hist: 4×4×8 histogram array (flattened, row-major)
    - x, y: Normalized patch coordinates (scaled by kp.sigma)
    - contrib: Weighted gradient magnitude
    - theta_mn: Relative orientation angle
    - lambda_desc: Descriptor patch scale factor
*/
void update_histogram_linear(float *hist, float x, float y, float contrib,
                             float theta_mn, float lambda_desc) {
  // Match CPU implementation: iterate all bins and compute distance-based weights
  // CPU code at lines 1705-1729 in sift.cpp
  
  float x_i, y_j;
  for (int i = 1; i <= N_HIST; i++) {
    // Compute bin center position
    x_i = (i - (1.0f + (float)N_HIST) / 2.0f) * 2.0f * lambda_desc / N_HIST;
    
    // Skip if too far from sample point
    if (fabs(x_i - x) > 2.0f * lambda_desc / N_HIST)
      continue;
    
    for (int j = 1; j <= N_HIST; j++) {
      y_j = (j - (1.0f + (float)N_HIST) / 2.0f) * 2.0f * lambda_desc / N_HIST;
      
      if (fabs(y_j - y) > 2.0f * lambda_desc / N_HIST)
        continue;
      
      // Compute spatial histogram weight (linear falloff)
      float hist_weight = (1.0f - N_HIST * 0.5f / lambda_desc * fabs(x_i - x)) *
                          (1.0f - N_HIST * 0.5f / lambda_desc * fabs(y_j - y));
      
      for (int k = 1; k <= N_ORI; k++) {
        float theta_k = TWO_PI * (k - 1) / N_ORI;
        
        // Compute orientation difference (wrapped to [0, 2π])
        float theta_diff = theta_k - theta_mn;
        while (theta_diff < 0.0f)
          theta_diff += TWO_PI;
        while (theta_diff >= TWO_PI)
          theta_diff -= TWO_PI;
        
        // Skip if orientation too far
        if (fabs(theta_diff) >= TWO_PI / N_ORI)
          continue;
        
        // Compute orientation histogram weight (linear falloff)
        float bin_weight = 1.0f - N_ORI * 0.5f / M_PI_F * fabs(theta_diff);
        
        // Accumulate contribution
        int hist_idx = (i - 1) * N_HIST * N_ORI + (j - 1) * N_ORI + (k - 1);
        hist[hist_idx] += hist_weight * bin_weight * contrib;
      }
    }
  }
}

/*
Main kernel: Compute 128-dimensional SIFT descriptors for oriented keypoints.

Algorithm:
    1. For each keypoint, sample 16×16 patch rotated by orientation angle
    2. Build 4×4×8 histogram via trilinear interpolation
    3. Flatten to 128-element vector
    4. Normalize to unit length
    5. Threshold at 0.2 and re-normalize (illumination invariance)
    6. Quantize to uint8 [0, 255]

Kernel parameters:
    - oriented_keypoints: Input oriented keypoints [OrientedKeypoint]
    - num_oriented: Number of oriented keypoints
    - grad_pyramid_buffer: Gradient pyramid (magnitude + orientation
interleaved)
    - width, height: Gradient image dimensions
    - lambda_desc: Descriptor patch scale factor (typically 1.5)
    - min_pix_dist: Minimum pixel distance (0.5)
    - descriptors: Output 128-dim descriptors [uint8 × 128 per keypoint]

Work-item organization:
    - Global work size: num_oriented (one thread per keypoint)
    - Each thread computes full 128-dim descriptor independently
*/
__kernel void
compute_descriptors(__global const OrientedKeypoint *oriented_keypoints,
                    int num_oriented, __global const float *grad_pyramid_buffer,
                    int width, int height, float lambda_desc,
                    float min_pix_dist, __global uchar *descriptors) {
  int kp_idx = get_global_id(0);
  if (kp_idx >= num_oriented)
    return;

  // Load oriented keypoint
  OrientedKeypoint okp = oriented_keypoints[kp_idx];
  GPUKeypoint kp = okp.kp;
  float theta = okp.orientation;

  // Compute pixel distance at this octave
  float pix_dist = min_pix_dist * pown(2.0f, kp.octave);

  // Patch parameters (match CPU lines 1764-1773)
  float patch_sigma = lambda_desc * kp.sigma;
  
  // Compute adaptive patch bounds (matches CPU implementation)
  // CPU: half_size = sqrt(2) * lambda_desc * kp.sigma * (N_HIST + 1) / N_HIST
  float half_size = M_SQRT2_F * lambda_desc * kp.sigma * (N_HIST + 1.0f) / N_HIST;
  int x_start = (int)round((kp.x - half_size) / pix_dist);
  int x_end = (int)round((kp.x + half_size) / pix_dist);
  int y_start = (int)round((kp.y - half_size) / pix_dist);
  int y_end = (int)round((kp.y + half_size) / pix_dist);

  // Build 4×4×8 histogram in local memory
  float hist[N_HIST * N_HIST * N_ORI];
  for (int i = 0; i < N_HIST * N_HIST * N_ORI; i++)
    hist[i] = 0.0f;

  // Precompute rotation matrix (match CPU cos_t, sin_t)
  float cos_t = native_cos(theta);
  float sin_t = native_sin(theta);

  // Sample adaptive patch (match CPU loop at lines 1778-1799)
  for (int m = x_start; m <= x_end; m++) {
    for (int n = y_start; n <= y_end; n++) {
      // Check image bounds
      if (m < 0 || m >= width || n < 0 || n >= height)
        continue;

      // Compute normalized coordinates relative to keypoint (match CPU line 1783-1784)
      // CPU: x = ((m*pix_dist - kp.x)*cos_t + (n*pix_dist - kp.y)*sin_t) / kp.sigma
      float x_norm = ((m * pix_dist - kp.x) * cos_t + 
                      (n * pix_dist - kp.y) * sin_t) / kp.sigma;
      float y_norm = (-(m * pix_dist - kp.x) * sin_t + 
                      (n * pix_dist - kp.y) * cos_t) / kp.sigma;

      // Verify (x, y) is inside the description patch (match CPU line 1787)
      if (fmax(fabs(x_norm), fabs(y_norm)) > lambda_desc * (N_HIST + 1.0f) / N_HIST)
        continue;

      // Sample gradients at pixel (m, n) - no interpolation, discrete pixels like CPU
      // CPU: gx = img_grad.get_pixel(m, n, 0), gy = img_grad.get_pixel(m, n, 1)
      float gx = grad_pyramid_buffer[2 * (n * width + m) + 0];
      float gy = grad_pyramid_buffer[2 * (n * width + m) + 1];

      // Compute orientation and magnitude (match CPU lines 1789-1790)
      float theta_mn = atan2(gy, gx) - theta;
      
      // Wrap to [0, 2π] range
      while (theta_mn < 0.0f)
        theta_mn += TWO_PI;
      while (theta_mn >= TWO_PI)
      while (theta_rel > M_PI_F)
        theta_rel -= TWO_PI;
      while (theta_rel < -M_PI_F)
        theta_rel += TWO_PI;

      // Gaussian weighting (distance from patch center)
      float dist_sq = x * x + y * y;
      float weight = native_exp(-dist_sq / (2.0f * patch_sigma * patch_sigma));

      // Trilinear interpolation into histogram
      float contrib = weight * grad_mag;
      update_histogram_trilinear(hist, (float)x, (float)y, contrib, theta_rel,
                                 patch_sigma);
    }
  }

  // Normalize histogram to unit length
  float norm = 0.0f;
  for (int i = 0; i < DESCRIPTOR_SIZE; i++)
    norm += hist[i] * hist[i];
  norm = native_sqrt(norm);

  if (norm < 1e-6f) {
    // Degenerate descriptor, set to zero
    for (int i = 0; i < DESCRIPTOR_SIZE; i++)
      descriptors[kp_idx * DESCRIPTOR_SIZE + i] = 0;
    return;
  }

  // First normalization
  for (int i = 0; i < DESCRIPTOR_SIZE; i++)
    hist[i] /= norm;

  // Threshold at 0.2 (illumination invariance)
  for (int i = 0; i < DESCRIPTOR_SIZE; i++)
    hist[i] = fmin(hist[i], 0.2f);

  // Re-normalize
  norm = 0.0f;
  for (int i = 0; i < DESCRIPTOR_SIZE; i++)
    norm += hist[i] * hist[i];
  norm = native_sqrt(norm);

  if (norm < 1e-6f)
    norm = 1.0f; // Safety

  // Quantize to uint8 [0, 255] and write output
  for (int i = 0; i < DESCRIPTOR_SIZE; i++) {
    float val = hist[i] / norm * 512.0f; // Scale to [0, 512]
    val = clamp(val, 0.0f, 255.0f);
    descriptors[kp_idx * DESCRIPTOR_SIZE + i] = (uchar)val;
  }
}

/*
USAGE PATTERN (Host Code in sift.cpp):

    // After compute_orientations kernel
    cl_mem descriptor_buffer = opencl_api.create_buffer(num_oriented * 128 *
sizeof(uint8_t), ...);

    cl_kernel kernel = opencl_api.get_kernel("compute_descriptors");

    // Launch for each octave (or all at once if octave info embedded)
    for (int oct = 0; oct < num_octaves; oct++)
    {
        auto [width, height] = grad_pyramid.get_dimensions(oct);

        clSetKernelArg(kernel, 0, sizeof(cl_mem), &oriented_buffer);
        clSetKernelArg(kernel, 1, sizeof(int), &num_oriented);
        clSetKernelArg(kernel, 2, sizeof(cl_mem), &grad_pyramid_buffer);
        clSetKernelArg(kernel, 3, sizeof(int), &width);
        clSetKernelArg(kernel, 4, sizeof(int), &height);
        clSetKernelArg(kernel, 5, sizeof(float), &lambda_desc);
        clSetKernelArg(kernel, 6, sizeof(float), &min_pix_dist);
        clSetKernelArg(kernel, 7, sizeof(cl_mem), &descriptor_buffer);

        size_t global_work_size = num_oriented;
        opencl_api.enqueue_kernel(kernel, 1, &global_work_size, nullptr);
    }

    // FINAL DOWNLOAD: Single D2H transfer of all descriptors!
    std::vector<uint8_t> host_descriptors(num_oriented * 128);
    opencl_api.read_buffer(descriptor_buffer, num_oriented * 128,
host_descriptors.data(), CL_TRUE);

PERFORMANCE ANALYSIS:
    - Input: 7K oriented keypoints
    - Patch sampling: 7K × 256 samples = 1.8M samples
    - Bilinear interpolations: 1.8M × 2 (mag + ori) = 3.6M interpolations
    - Histogram updates: 1.8M × 8 (trilinear) = 14.4M atomic-free adds
    - Normalization: 7K × 2 passes × 128 = 1.8M FLOPs
    - Total FLOPs: ~60M (~2ms @ 30 TFLOPS)
    - Memory reads: 7K × 256 × 8 bytes = 14 MB (~0.09ms @ 150 GB/s)
    - Memory writes: 7K × 128 bytes = 896 KB (~0.006ms)
    - Expected time: ~8-12ms (vs ~30ms CPU) = 2.5-4× speedup

ZERO READBACK VALIDATION:
    Input: oriented_buffer (already in VRAM from compute_orientations)
    Gradient data: grad_pyramid (already in VRAM)
    Output: descriptor_buffer (FINAL GPU DATA - SINGLE D2H TRANSFER!)
    Result: NO intermediate H2D or D2H transfers!

RTX 4060 OCCUPANCY ANALYSIS:
    - Threads: 7K (LOW - only 28% of 24,576 CUDA cores)
    - Registers/thread: ~60 (histogram + temps)
    - Local memory: 512 bytes/thread (4×4×8 histogram)
    - Occupancy: ~25-30%
    - Strategy: Maximize ILP, use native math functions for throughput

NOTE: Low occupancy is acceptable here because:
    1. Workload size is data-dependent (7K keypoints is typical)
    2. High arithmetic intensity (60M FLOPs / 14 MB = 4.3 FLOPs/byte)
    3. Memory-bound rather than compute-bound (utilizes texture cache)
*/
